{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.losses as kls\n",
    "import tensorflow.keras.optimizers as ko\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"TensorFlow Ver: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilityDistribution(tf.keras.Model):\n",
    "  def call(self, logits, **kwargs):\n",
    "    # Sample a random categorical action from the given logits.\n",
    "    return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, num_actions):\n",
    "      \n",
    "    super().__init__('mlp_policy')\n",
    "\n",
    "    # Note: no tf.get_variable(), just simple Keras API!\n",
    "    self.hidden1 = kl.Dense(128, activation='relu')\n",
    "    self.hidden2 = kl.Dense(128, activation='relu')\n",
    "    self.value = kl.Dense(1, name='value')\n",
    "\n",
    "    # Logits are unnormalized log probabilities.\n",
    "    self.logits = kl.Dense(num_actions, name='policy_logits')\n",
    "    self.dist = ProbabilityDistribution()\n",
    "\n",
    "  def call(self, inputs, **kwargs):\n",
    "    # Inputs is a numpy array, convert to a tensor.\n",
    "    x = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    # Separate hidden layers from the same input tensor.\n",
    "    hidden_logs = self.hidden1(x)\n",
    "    hidden_vals = self.hidden2(x)\n",
    "    return self.logits(hidden_logs), self.value(hidden_vals)\n",
    "\n",
    "  def action_value(self, obs):\n",
    "    # Executes `call()` under the hood.\n",
    "    logits, value = self.predict_on_batch(obs)\n",
    "    action = self.dist.predict_on_batch(logits)\n",
    "\n",
    "    # Another way to sample actions:\n",
    "    #   action = tf.random.categorical(logits, 1)\n",
    "    # Will become clearer later why we don't use it.\n",
    "    return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify everything works by sampling a single action.\n",
    "env = gym.make('CartPole-v0')\n",
    "model = Model(num_actions=env.action_space.n)\n",
    "model.action_value(env.reset()[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "  def __init__(self, model, lr=7e-4, gamma=0.99, value_c=0.5, entropy_c=1e-4):\n",
    "        # `gamma` is the discount factor; coefficients are used for the loss terms.\n",
    "        self.gamma = gamma\n",
    "        self.value_c = value_c\n",
    "        self.entropy_c = entropy_c\n",
    "\n",
    "        self.model = model\n",
    "        self.model.compile(\n",
    "        optimizer=ko.RMSprop(lr=lr),\n",
    "        # Define separate losses for policy logits and value estimate.\n",
    "        loss=[self._logits_loss, self._value_loss])\n",
    "\n",
    "  def train(self, env, batch_sz=64, updates=250):\n",
    "        # Storage helpers for a single batch of data.\n",
    "        actions = np.empty((batch_sz,), dtype=np.int32)\n",
    "        rewards, dones, values = np.empty((3, batch_sz))\n",
    "        observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
    "\n",
    "        # Training loop: collect samples, send to optimizer, repeat updates times.\n",
    "        ep_rewards = [0.0]\n",
    "        next_obs = env.reset()\n",
    "        for update in range(updates):\n",
    "            for step in range(batch_sz):\n",
    "\n",
    "                  observations[step] = next_obs.copy()\n",
    "                  actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
    "                  next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
    "                  ep_rewards[-1] += rewards[step]\n",
    "\n",
    "                  if dones[step]:\n",
    "                        ep_rewards.append(0.0)\n",
    "                        next_obs = env.reset()\n",
    "                        logging.info(\"Episode: %03d, Reward: %03d\" % (len(ep_rewards) - 1, ep_rewards[-2]))\n",
    "\n",
    "        _, next_value = self.model.action_value(next_obs[None, :])\n",
    "        returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
    "\n",
    "        # A trick to input actions and advantages through same API.\n",
    "        acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "\n",
    "        # Performs a full training step on the collected batch.\n",
    "        # Note: no need to mess around with gradients, Keras API handles it.\n",
    "        print( observations.shape )\n",
    "        print( next_obs.shape )\n",
    "        losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
    "        logging.debug(\"[%d/%d] Losses: %s\" % (update + 1, updates, losses))\n",
    "\n",
    "        return ep_rewards\n",
    "\n",
    "  def _returns_advantages(self, rewards, dones, values, next_value):\n",
    "        # `next_value` is the bootstrap value estimate of the future state (critic).\n",
    "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "\n",
    "        # Returns are calculated as discounted sum of future rewards.\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\n",
    "\n",
    "        returns = returns[:-1]\n",
    "\n",
    "        # Advantages are equal to returns - baseline (value estimates in our case).\n",
    "        advantages = returns - values\n",
    "\n",
    "        return returns, advantages\n",
    "\n",
    "  def _value_loss(self, returns, value):\n",
    "        # Value loss is typically MSE between value estimates and returns.\n",
    "        return self.value_c * kls.mean_squared_error(returns, value)\n",
    "\n",
    "  def _logits_loss(self, actions_and_advantages, logits):\n",
    "        # A trick to input actions and advantages through the same API.\n",
    "        actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n",
    "\n",
    "        # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n",
    "        # `from_logits` argument ensures transformation into normalized probabilities.\n",
    "        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        # Policy loss is defined by policy gradients, weighted by advantages.\n",
    "        # Note: we only calculate the loss on the actions we've actually taken.\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
    "\n",
    "        # Entropy loss can be calculated as cross-entropy over itself.\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        entropy_loss = kls.categorical_crossentropy(probs, probs)\n",
    "\n",
    "        # We want to minimize policy and maximize entropy losses.\n",
    "        # Here signs are flipped because the optimizer minimizes.\n",
    "        return policy_loss - self.entropy_c * entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set to logging.WARNING to disable logs or logging.DEBUG to see losses as well\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "model = Model(num_actions=env.action_space.n)\n",
    "agent = A2CAgent(model)\n",
    "\n",
    "rewards_history = agent.train(env)\n",
    "print(\"Finished training! Testing...\")\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(np.arange(0, len(rewards_history), 5), rewards_history[::5])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594924200133",
   "display_name": "Python 3.7.7 64-bit ('tf2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}